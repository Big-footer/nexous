name: Performance Benchmark

on:
  schedule:
    - cron: '0 2 * * *'  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ (UTC)
  workflow_dispatch:  # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥

env:
  PROJECT_PATH: projects/flood_analysis_ulsan/project.yaml
  BENCHMARK_RUNS: 5

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
      
      - name: Run benchmark (Mock mode)
        run: |
          echo "ðŸƒ Running ${{ env.BENCHMARK_RUNS }} benchmark iterations"
          
          for i in $(seq 1 ${{ env.BENCHMARK_RUNS }}); do
            RUN_ID="benchmark_$(date +%Y%m%d)_${i}"
            echo "Run $i: $RUN_ID"
            
            python -m nexous.cli.main run \
              ${{ env.PROJECT_PATH }} \
              --trace-dir traces \
              --run-id "$RUN_ID"
          done
      
      - name: Analyze benchmark results
        run: |
          echo "ðŸ“Š Benchmark Results Analysis"
          echo ""
          
          # Duration ì¶”ì¶œ ë° í†µê³„
          DURATIONS=$(jq '.duration_ms' traces/flood_analysis_ulsan/benchmark_$(date +%Y%m%d)_*/trace.json)
          
          echo "Durations (ms):"
          echo "$DURATIONS"
          
          # í‰ê·  ê³„ì‚°
          AVG=$(echo "$DURATIONS" | awk '{sum+=$1} END {print sum/NR}')
          echo ""
          echo "Average: ${AVG}ms"
          
          # ìµœì†Œ/ìµœëŒ€
          MIN=$(echo "$DURATIONS" | sort -n | head -1)
          MAX=$(echo "$DURATIONS" | sort -n | tail -1)
          echo "Min: ${MIN}ms"
          echo "Max: ${MAX}ms"
          
          # ê²°ê³¼ ì €ìž¥
          cat > benchmark_results.json << EOF
          {
            "date": "$(date -I)",
            "runs": ${{ env.BENCHMARK_RUNS }},
            "average_ms": $AVG,
            "min_ms": $MIN,
            "max_ms": $MAX,
            "durations": [$DURATIONS]
          }
          EOF
          
          cat benchmark_results.json
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.run_number }}
          path: |
            traces/flood_analysis_ulsan/benchmark_*
            benchmark_results.json
          retention-days: 90
      
      - name: Compare with previous benchmark
        continue-on-error: true
        run: |
          # ì´ì „ benchmarkì™€ ë¹„êµ ë¡œì§
          # (ì´ì „ artifact ë‹¤ìš´ë¡œë“œ í•„ìš”)
          echo "ðŸ“ˆ Comparison with previous benchmark"
          echo "TODO: Implement historical comparison"
