"""
PROMETHEUS LangGraph ë…¸ë“œ í•¨ìˆ˜ (LangChain Agent í†µí•©)

ê° Agentì˜ ì‹¤í–‰ ë¡œì§ì„ ì •ì˜í•©ë‹ˆë‹¤.
LangChain Agentì™€ ì—°ë™í•˜ì—¬ ì‹¤ì œ LLMì„ í˜¸ì¶œí•©ë‹ˆë‹¤.

ê°œì„ ì‚¬í•­:
- LLM Factory ì‚¬ìš©ìœ¼ë¡œ Fallback/Retry ìë™ ì ìš©
- ì¬ì‹œë„ íšŸìˆ˜ Stateì—ì„œ ê´€ë¦¬
"""

from typing import Dict, Any, Literal, Optional
from datetime import datetime
import json
import os
import logging

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from prometheus.graphs.state import AgentState
from prometheus.llm import (
    get_llm_factory,
    create_robust_llm,
    get_llm,  # í•˜ìœ„ í˜¸í™˜ì„±
    LLMFactory,
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì „ì—­ LLM Factory
_llm_factory: Optional[LLMFactory] = None


def get_node_llm_factory() -> LLMFactory:
    """ë…¸ë“œìš© LLM Factory ë°˜í™˜"""
    global _llm_factory
    if _llm_factory is None:
        _llm_factory = get_llm_factory()
    return _llm_factory


def set_node_llm_factory(factory: LLMFactory):
    """ë…¸ë“œìš© LLM Factory ì„¤ì •"""
    global _llm_factory
    _llm_factory = factory


# =============================================================================
# Meta Agent ë…¸ë“œ
# =============================================================================

def meta_agent_node(state: AgentState) -> Dict[str, Any]:
    """
    Meta Agent: ìš”ì²­ ë¶„ì„ ë° Agent/LLM ì„ íƒ
    
    LangChain Agentë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì²­ì„ ë¶„ì„í•˜ê³ 
    ê° Agentì— ì‚¬ìš©í•  LLMì„ ê²°ì •í•©ë‹ˆë‹¤.
    
    Fallback: Claude â†’ GPT â†’ Gemini
    """
    logger.info("ğŸ” Meta Agent ì‹œì‘")
    
    request = state["request"]
    retry_count = state.get("retry_count", 0)
    
    # Fallbackì´ ì„¤ì •ëœ LLM ì‚¬ìš©
    factory = get_node_llm_factory()
    llm = factory.get_meta_llm()
    
    system_prompt = """ë‹¹ì‹ ì€ PROMETHEUSì˜ Meta-Agentì…ë‹ˆë‹¤.
ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ Agentì™€ LLM ì¡°í•©ì„ ê²°ì •í•©ë‹ˆë‹¤.

ë‹¤ìŒ Agentë“¤ì´ ìˆìŠµë‹ˆë‹¤:
- planner: ì‘ì—… ê³„íš ìˆ˜ë¦½ (ë³µì¡í•œ ì¶”ë¡  í•„ìš” â†’ Claude ì¶”ì²œ)
- executor: ì½”ë“œ ì‹¤í–‰, Tool í˜¸ì¶œ (ì •í™•í•œ ì‹¤í–‰ í•„ìš” â†’ GPT ì¶”ì²œ)
- writer: ë¬¸ì„œ/ë³´ê³ ì„œ ì‘ì„± (ìì—°ìŠ¤ëŸ¬ìš´ ê¸€ â†’ Gemini ì¶”ì²œ)
- qa: í’ˆì§ˆ ê²€í†  (ê¼¼ê¼¼í•œ ê²€í†  â†’ Claude ì¶”ì²œ)

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "selected_agents": ["planner", "executor", "writer"],
    "llm_assignments": {
        "planner": "anthropic",
        "executor": "openai",
        "writer": "google"
    },
    "skip_qa": false,
    "reasoning": "ì´ìœ  ì„¤ëª…"
}
"""
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ë‹¤ìŒ ìš”ì²­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{request}")
    ]
    
    try:
        response = llm.invoke(messages)
        
        # JSON íŒŒì‹±
        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        elif "```" in content:
            content = content.split("```")[1].split("```")[0]
        
        decision = json.loads(content.strip())
        logger.info(f"âœ… Meta Agent ê²°ì •: {decision.get('reasoning', '')[:50]}...")
        
    except Exception as e:
        logger.warning(f"âš ï¸ Meta Agent ì˜¤ë¥˜ (ì‹œë„ {retry_count + 1}): {e}")
        decision = {
            "selected_agents": ["planner", "executor", "writer"],
            "llm_assignments": {
                "planner": "anthropic",
                "executor": "openai",
                "writer": "google"
            },
            "skip_qa": False,
            "reasoning": f"ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ì ìš© (fallback, ì˜¤ë¥˜: {str(e)[:50]})"
        }
    
    return {
        "meta_decision": decision,
        "current_agent": "meta",
        "messages": [AIMessage(content=f"Meta Agent ê²°ì • ì™„ë£Œ: {decision.get('reasoning', '')}")]
    }


# =============================================================================
# Planner Agent ë…¸ë“œ
# =============================================================================

def planner_node(state: AgentState) -> Dict[str, Any]:
    """
    Planner Agent: ì‘ì—… ê³„íš ìˆ˜ë¦½
    
    LangChain PlannerAgentë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„íšì„ ìƒì„±í•©ë‹ˆë‹¤.
    Fallback: Meta ê²°ì • provider â†’ ëŒ€ì²´ provider
    """
    logger.info("ğŸ“‹ Planner Agent ì‹œì‘")
    
    request = state["request"]
    meta_decision = state.get("meta_decision", {})
    retry_count = state.get("retry_count", 0)
    
    # LLM ì„ íƒ (Meta ê²°ì • ë˜ëŠ” Factory)
    provider = meta_decision.get("llm_assignments", {}).get("planner", "anthropic")
    
    try:
        # LangChain PlannerAgent ì‚¬ìš©
        from prometheus.agents import create_planner_agent
        from prometheus.llm import create_llm
        
        # ê¸°ë³¸ LLM ìƒì„± (Retry ì—†ì´ - with_structured_output í˜¸í™˜ì„±)
        llm = create_llm(provider=provider)
        
        planner = create_planner_agent(llm=llm)
        plan_output = planner.plan(request)
        
        # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
        plan = plan_output.model_dump() if hasattr(plan_output, 'model_dump') else dict(plan_output)
        
        logger.info(f"âœ… Planner ì™„ë£Œ: {plan.get('task_summary', '')[:50]}...")
        
    except Exception as e:
        logger.error(f"âŒ Planner ì˜¤ë¥˜ (ì‹œë„ {retry_count + 1}): {e}")
        # í´ë°±: Robust LLMìœ¼ë¡œ ì§ì ‘ í˜¸ì¶œ
        plan = _fallback_planner(request, provider)
    
    return {
        "plan": plan,
        "current_agent": "planner",
        "current_step": 0,
        "messages": [AIMessage(content=f"ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {plan.get('task_summary', '')}")]
    }


def _fallback_planner(request: str, provider: str) -> Dict[str, Any]:
    """Planner í´ë°± (Robust LLM ì§ì ‘ í˜¸ì¶œ)"""
    # Fallbackì´ ì„¤ì •ëœ LLM ì‚¬ìš©
    llm = create_robust_llm(provider, with_fallback=True, max_retries=2)
    
    system_prompt = """ë‹¹ì‹ ì€ Plannerì…ë‹ˆë‹¤. ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:
{
    "task_summary": "ì‘ì—… ìš”ì•½",
    "analysis": "ë¶„ì„",
    "steps": [{"step_id": 1, "action": "ì‘ì—…", "tool": null}],
    "total_steps": 1,
    "estimated_time": "5ë¶„"
}"""
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=request)
    ]
    
    response = llm.invoke(messages)
    
    try:
        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        return json.loads(content.strip())
    except:
        return {
            "task_summary": request[:100],
            "analysis": "ê¸°ë³¸ ë¶„ì„",
            "steps": [{"step_id": 1, "action": "ìš”ì²­ ì²˜ë¦¬", "tool": None}],
            "total_steps": 1,
            "estimated_time": "ì•Œ ìˆ˜ ì—†ìŒ"
        }


# =============================================================================
# Executor Agent ë…¸ë“œ
# =============================================================================

def executor_node(state: AgentState) -> Dict[str, Any]:
    """
    Executor Agent: ê³„íš ì‹¤í–‰
    
    LangChain ExecutorAgentë¥¼ ì‚¬ìš©í•˜ì—¬ Toolì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    """
    logger.info("âš¡ Executor Agent ì‹œì‘")
    
    plan = state.get("plan", {})
    meta_decision = state.get("meta_decision", {})
    retry_count = state.get("retry_count", 0)
    
    # LLM ì„ íƒ
    provider = meta_decision.get("llm_assignments", {}).get("executor", "openai")
    
    try:
        # LangChain ExecutorAgent ì‚¬ìš©
        from prometheus.agents import create_executor_agent
        
        executor = create_executor_agent(provider=provider)
        exec_result = executor.execute_plan(plan)
        
        # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
        execution_result = exec_result.model_dump() if hasattr(exec_result, 'model_dump') else dict(exec_result)
        
        logger.info(f"âœ… Executor ì™„ë£Œ: {execution_result.get('summary', '')}")
        
    except Exception as e:
        logger.error(f"âŒ Executor ì˜¤ë¥˜: {e}")
        # í´ë°±
        execution_result = _fallback_executor(plan, provider)
    
    return {
        "execution_result": execution_result,
        "current_agent": "executor",
        "retry_count": retry_count,
        "messages": [AIMessage(content=f"ì‹¤í–‰ ì™„ë£Œ: {execution_result.get('success_count', 0)}ê°œ ì„±ê³µ")]
    }


def _fallback_executor(plan: Dict[str, Any], provider: str) -> Dict[str, Any]:
    """Executor í´ë°±"""
    steps = plan.get("steps", [])
    step_results = []
    
    for step in steps:
        step_results.append({
            "step_id": step.get("step_id", 0),
            "action": step.get("action", ""),
            "status": "success",
            "output": f"Step {step.get('step_id', 0)} ì™„ë£Œ",
            "tool_calls": [],
            "error": None
        })
    
    return {
        "step_results": step_results,
        "success_count": len(step_results),
        "fail_count": 0,
        "artifacts": [],
        "total_execution_time": 0.0,
        "summary": f"ì´ {len(step_results)}ê°œ ë‹¨ê³„ ì‹¤í–‰ ì™„ë£Œ"
    }


# =============================================================================
# Writer Agent ë…¸ë“œ
# =============================================================================

def writer_node(state: AgentState) -> Dict[str, Any]:
    """
    Writer Agent: ë³´ê³ ì„œ ì‘ì„±
    
    LangChain WriterAgentë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    logger.info("âœï¸ Writer Agent ì‹œì‘")
    
    request = state["request"]
    plan = state.get("plan", {})
    execution_result = state.get("execution_result", {})
    meta_decision = state.get("meta_decision", {})
    
    # LLM ì„ íƒ
    provider = meta_decision.get("llm_assignments", {}).get("writer", "google")
    
    try:
        # LangChain WriterAgent ì‚¬ìš©
        from prometheus.agents import create_writer_agent
        
        writer = create_writer_agent(provider=provider)
        report_output = writer.write_report(request, plan, execution_result)
        
        # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
        report = report_output.model_dump() if hasattr(report_output, 'model_dump') else dict(report_output)
        
        logger.info(f"âœ… Writer ì™„ë£Œ: {report.get('title', '')}")
        
    except Exception as e:
        logger.error(f"âŒ Writer ì˜¤ë¥˜: {e}")
        # í´ë°±
        report = _fallback_writer(request, plan, execution_result, provider)
    
    return {
        "report": report,
        "current_agent": "writer",
        "messages": [AIMessage(content=f"ë³´ê³ ì„œ ì‘ì„± ì™„ë£Œ: {report.get('title', '')}")]
    }


def _fallback_writer(request: str, plan: Dict, execution_result: Dict, provider: str) -> Dict[str, Any]:
    """Writer í´ë°±"""
    llm = get_llm(provider)
    
    system_prompt = """ë‹¹ì‹ ì€ Writerì…ë‹ˆë‹¤. ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:
{
    "title": "ë³´ê³ ì„œ ì œëª©",
    "summary": "ìš”ì•½",
    "content": "ë³¸ë¬¸ (Markdown)",
    "conclusions": ["ê²°ë¡ 1"],
    "recommendations": [],
    "citations": [],
    "word_count": 100
}"""
    
    context = f"ìš”ì²­: {request}\nê³„íš: {json.dumps(plan, ensure_ascii=False)}\nê²°ê³¼: {json.dumps(execution_result, ensure_ascii=False)}"
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=context)
    ]
    
    try:
        response = llm.invoke(messages)
        content = response.content
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0]
        return json.loads(content.strip())
    except:
        return {
            "title": "ì‹¤í–‰ ê²°ê³¼ ë³´ê³ ì„œ",
            "summary": plan.get("task_summary", "ì‘ì—… ì™„ë£Œ"),
            "content": f"# ê²°ê³¼\n\n{str(execution_result)}",
            "conclusions": ["ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."],
            "recommendations": [],
            "citations": [],
            "word_count": 100
        }


# =============================================================================
# QA Agent ë…¸ë“œ
# =============================================================================

def qa_node(state: AgentState) -> Dict[str, Any]:
    """
    QA Agent: í’ˆì§ˆ ê²€í† 
    
    LangChain QAAgentë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ê³ ì„œë¥¼ ê²€í† í•©ë‹ˆë‹¤.
    """
    logger.info("ğŸ” QA Agent ì‹œì‘")
    
    request = state["request"]
    report = state.get("report", {})
    execution_result = state.get("execution_result", {})
    meta_decision = state.get("meta_decision", {})
    
    # QA ìŠ¤í‚µ ì²´í¬
    if meta_decision.get("skip_qa", False):
        logger.info("â­ï¸ QA ìŠ¤í‚µë¨")
        return {
            "qa_result": {
                "passed": True,
                "score": 100.0,
                "grade": "A",
                "summary": "QA ìŠ¤í‚µë¨",
                "issues": [],
                "strengths": [],
                "recommendations": []
            },
            "current_agent": "qa",
            "messages": [AIMessage(content="QA ìŠ¤í‚µë¨")]
        }
    
    try:
        # LangChain QAAgent ì‚¬ìš©
        from prometheus.agents import create_qa_agent
        
        qa = create_qa_agent(provider="anthropic")
        qa_output = qa.review(request, report, execution_result)
        
        # Pydantic ëª¨ë¸ì„ dictë¡œ ë³€í™˜
        qa_result = qa_output.model_dump() if hasattr(qa_output, 'model_dump') else dict(qa_output)
        
        logger.info(f"âœ… QA ì™„ë£Œ: {qa_result.get('grade', '')} ({qa_result.get('score', 0)}ì )")
        
    except Exception as e:
        logger.error(f"âŒ QA ì˜¤ë¥˜: {e}")
        # í´ë°±
        qa_result = {
            "passed": True,
            "score": 75.0,
            "grade": "C",
            "summary": "ê¸°ë³¸ ê²€í†  ì™„ë£Œ",
            "issues": [],
            "strengths": ["ì‘ì—… ì™„ë£Œ"],
            "recommendations": ["ìƒì„¸ ê²€í†  ê¶Œì¥"]
        }
    
    return {
        "qa_result": qa_result,
        "current_agent": "qa",
        "messages": [AIMessage(content=f"QA ì™„ë£Œ: {qa_result.get('grade', '')} ({qa_result.get('score', 0)}ì )")]
    }


# =============================================================================
# ì—ëŸ¬ í•¸ë“¤ëŸ¬ ë…¸ë“œ
# =============================================================================

def error_handler_node(state: AgentState) -> Dict[str, Any]:
    """ì—ëŸ¬ ì²˜ë¦¬ ë…¸ë“œ"""
    logger.error("âŒ ì—ëŸ¬ í•¸ë“¤ëŸ¬ í˜¸ì¶œë¨")
    
    return {
        "error": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ë˜ëŠ” ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ",
        "current_agent": "error",
        "messages": [AIMessage(content="ì—ëŸ¬ ë°œìƒ: ì›Œí¬í”Œë¡œìš°ë¥¼ ì™„ë£Œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")]
    }


# =============================================================================
# ë¼ìš°íŒ… í•¨ìˆ˜
# =============================================================================

def should_run_qa(state: AgentState) -> Literal["qa", "end"]:
    """QA ì‹¤í–‰ ì—¬ë¶€ ê²°ì •"""
    meta_decision = state.get("meta_decision", {})
    
    if meta_decision.get("skip_qa", False):
        logger.info("â¡ï¸ QA ìŠ¤í‚µ â†’ END")
        return "end"
    
    logger.info("â¡ï¸ QA ì‹¤í–‰")
    return "qa"


def should_retry_executor(state: AgentState) -> Literal["executor", "writer", "error"]:
    """Executor ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
    execution_result = state.get("execution_result", {})
    retry_count = state.get("retry_count", 0)
    
    fail_count = execution_result.get("fail_count", 0)
    
    if fail_count > 0 and retry_count < 3:
        logger.info(f"ğŸ”„ Executor ì¬ì‹œë„ ({retry_count + 1}/3)")
        return "executor"
    elif fail_count > 0 and retry_count >= 3:
        logger.error("âŒ ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼ â†’ ì—ëŸ¬")
        return "error"
    
    logger.info("â¡ï¸ Writerë¡œ ì§„í–‰")
    return "writer"
